{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyO5t2rd4NgrnlNWGPbXVqAI",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Anurag-Raj-Shukla/Chess-Engine/blob/main/Next_Word_N_gram.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# NLP Trigram next word/phrase suggestor\n"
      ],
      "metadata": {
        "id": "cLo1yf-ikCzx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0399623a"
      },
      "source": [
        "# Task\n",
        "Build a trigram model using the wikipedia library to predict the next word/phrase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cc598e7e"
      },
      "source": [
        "## Install wikipedia library\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary library using pip.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba6dc5aa"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to install the `wikipedia` library. I will use the `pip` command with the `install` option to accomplish this.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62c808aa",
        "outputId": "3b402a5c-0892-4d74-a744-a1b4d7c90fca"
      },
      "source": [
        "%pip install wikipedia"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wikipedia in /usr/local/lib/python3.12/dist-packages (1.4.0)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (4.13.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from wikipedia) (2.32.4)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.0.0->wikipedia) (2025.8.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (2.8)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.12/dist-packages (from beautifulsoup4->wikipedia) (4.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22e03685"
      },
      "source": [
        "## Load and preprocess data\n",
        "\n",
        "### Subtask:\n",
        "Load the text data using the wikipedia library and preprocess it by converting to lowercase, removing punctuation, and tokenizing into words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9ae3cd7"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the text data using the wikipedia library, convert it to lowercase, remove punctuation, and tokenize it into words.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "013b10f4",
        "outputId": "6dc46aa0-03a1-49b9-cbe8-b70b230229cc"
      },
      "source": [
        "import wikipedia\n",
        "import string\n",
        "\n",
        "# Search for a Wikipedia page and get its content\n",
        "page_title = \"Artificial intelligence\"\n",
        "try:\n",
        "    page_content = wikipedia.page(page_title).content\n",
        "except wikipedia.exceptions.PageError:\n",
        "    print(f\"Page '{page_title}' not found.\")\n",
        "    page_content = \"\"\n",
        "except wikipedia.exceptions.DisambiguationError as e:\n",
        "    print(f\"Disambiguation page for '{page_title}'. Options: {e.options}\")\n",
        "    page_content = wikipedia.page(e.options[0]).content\n",
        "\n",
        "\n",
        "# Convert to lowercase\n",
        "lowercase_text = page_content.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "cleaned_text = lowercase_text.translate(translator)\n",
        "\n",
        "# Tokenize into words\n",
        "words = cleaned_text.split()\n",
        "\n",
        "print(f\"First 20 tokens: {words[:20]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 20 tokens: ['artificial', 'intelligence', 'ai', 'is', 'the', 'capability', 'of', 'computational', 'systems', 'to', 'perform', 'tasks', 'typically', 'associated', 'with', 'human', 'intelligence', 'such', 'as', 'learning']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "049f0c19"
      },
      "source": [
        "## Create trigrams\n",
        "\n",
        "### Subtask:\n",
        "Generate trigrams (sequences of three consecutive words) from the tokenized text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "63686757"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate trigrams from the tokenized words list and print the first 10 trigrams.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "651603d7",
        "outputId": "a75816bd-290f-4664-e0e7-ce7c064cb969"
      },
      "source": [
        "trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    trigram = (words[i], words[i+1], words[i+2])\n",
        "    trigrams.append(trigram)\n",
        "\n",
        "print(\"First 10 trigrams:\")\n",
        "for trigram in trigrams[:10]:\n",
        "    print(trigram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 trigrams:\n",
            "('artificial', 'intelligence', 'ai')\n",
            "('intelligence', 'ai', 'is')\n",
            "('ai', 'is', 'the')\n",
            "('is', 'the', 'capability')\n",
            "('the', 'capability', 'of')\n",
            "('capability', 'of', 'computational')\n",
            "('of', 'computational', 'systems')\n",
            "('computational', 'systems', 'to')\n",
            "('systems', 'to', 'perform')\n",
            "('to', 'perform', 'tasks')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c842e7b"
      },
      "source": [
        "## Build trigram frequency table\n",
        "\n",
        "### Subtask:\n",
        "Build a trigram frequency table to count how many times each trigram appears.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aa98089"
      },
      "source": [
        "**Reasoning**:\n",
        "Import Counter and build the trigram frequency table using the list of trigrams, then print the 10 most common trigrams.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cd208b98",
        "outputId": "549d117c-24bf-4d35-c5af-41af60580641"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "trigram_frequencies = Counter(trigrams)\n",
        "\n",
        "print(\"10 most common trigrams:\")\n",
        "for trigram, count in trigram_frequencies.most_common(10):\n",
        "    print(f\"{trigram}: {count}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 most common trigrams:\n",
            "('of', 'artificial', 'intelligence'): 11\n",
            "('the', 'ability', 'to'): 7\n",
            "('artificial', 'general', 'intelligence'): 6\n",
            "('as', 'well', 'as'): 6\n",
            "('can', 'be', 'used'): 6\n",
            "('the', 'study', 'of'): 6\n",
            "('of', 'ai', 'research'): 5\n",
            "('the', 'use', 'of'): 5\n",
            "('be', 'able', 'to'): 5\n",
            "('a', 'set', 'of'): 5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6427c34"
      },
      "source": [
        "## Build next word prediction function\n",
        "\n",
        "### Subtask:\n",
        "Create a function that takes two words as input and predicts the next word based on the trigram frequencies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "868a4b53"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the function to predict the next word based on trigram frequencies.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bee5c3e5",
        "outputId": "ae531aa9-5c41-44f8-e50a-c4b98c874f50"
      },
      "source": [
        "def predict_next_word(word1, word2, trigram_frequencies):\n",
        "    \"\"\"\n",
        "    Predicts the next word based on trigram frequencies.\n",
        "\n",
        "    Args:\n",
        "        word1: The first word of the bigram.\n",
        "        word2: The second word of the bigram.\n",
        "        trigram_frequencies: A Counter object containing trigram frequencies.\n",
        "\n",
        "    Returns:\n",
        "        The most frequent third word that follows the bigram, or None if no\n",
        "        matching trigrams are found.\n",
        "    \"\"\"\n",
        "    matching_trigrams = {}\n",
        "    for trigram, frequency in trigram_frequencies.items():\n",
        "        if trigram[0] == word1 and trigram[1] == word2:\n",
        "            matching_trigrams[trigram[2]] = frequency\n",
        "\n",
        "    if matching_trigrams:\n",
        "        # Find the word with the highest frequency\n",
        "        predicted_word = max(matching_trigrams, key=matching_trigrams.get)\n",
        "        return predicted_word\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Example usage:\n",
        "word1 = \"artificial\"\n",
        "word2 = \"intelligence\"\n",
        "prediction = predict_next_word(word1, word2, trigram_frequencies)\n",
        "print(f\"Given '{word1} {word2}', the predicted next word is: {prediction}\")\n",
        "\n",
        "word1 = \"this\"\n",
        "word2 = \"is\"\n",
        "prediction = predict_next_word(word1, word2, trigram_frequencies)\n",
        "print(f\"Given '{word1} {word2}', the predicted next word is: {prediction}\")\n",
        "\n",
        "word1 = \"nonexistent\"\n",
        "word2 = \"bigram\"\n",
        "prediction = predict_next_word(word1, word2, trigram_frequencies)\n",
        "print(f\"Given '{word1} {word2}', the predicted next word is: {prediction}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given 'artificial intelligence', the predicted next word is: is\n",
            "Given 'this is', the predicted next word is: the\n",
            "Given 'nonexistent bigram', the predicted next word is: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1c27a5d"
      },
      "source": [
        "## Test the model\n",
        "\n",
        "### Subtask:\n",
        "Test the prediction function with some example word pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1265e5f5"
      },
      "source": [
        "**Reasoning**:\n",
        "Call the predict_next_word function with different word pairs and print the results to test its functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "51bb1c47",
        "outputId": "acd9483a-01e7-4928-f261-5325fc2da3f0"
      },
      "source": [
        "# Test the prediction function with some example word pairs\n",
        "test_cases = [\n",
        "    (\"artificial\", \"intelligence\"),\n",
        "    (\"machine\", \"learning\"),\n",
        "    (\"data\", \"science\"),\n",
        "    (\"this\", \"is\"),\n",
        "    (\"a\", \"set\"),\n",
        "    (\"apple\", \"banana\") # Example of words not expected to be found\n",
        "]\n",
        "\n",
        "for word1, word2 in test_cases:\n",
        "    prediction = predict_next_word(word1, word2, trigram_frequencies)\n",
        "    print(f\"Given '{word1} {word2}', the predicted next word is: {prediction}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given 'artificial intelligence', the predicted next word is: is\n",
            "Given 'machine learning', the predicted next word is: models\n",
            "Given 'data science', the predicted next word is: None\n",
            "Given 'this is', the predicted next word is: the\n",
            "Given 'a set', the predicted next word is: of\n",
            "Given 'apple banana', the predicted next word is: None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "592dedd5"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `wikipedia` library was successfully installed and used to load text data from the \"Artificial intelligence\" page.\n",
        "*   The loaded text was successfully preprocessed by converting it to lowercase, removing punctuation, and tokenizing it into words.\n",
        "*   Trigrams (sequences of three consecutive words) were successfully generated from the tokenized text.\n",
        "*   A trigram frequency table was built using `collections.Counter`, effectively counting the occurrences of each unique trigram.\n",
        "*   A function `predict_next_word` was created which takes two words as input and predicts the next word based on the trigram frequencies, returning the most frequent third word or `None` if no matching trigrams are found.\n",
        "*   Testing of the prediction function showed that it successfully predicted the next word for bigrams present in the training data (e.g., \"artificial intelligence\" predicted \"is\") and returned `None` for bigrams not found in the data (e.g., \"apple banana\").\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The current model uses simple frequency counts. Exploring smoothing techniques (like Add-One smoothing) could improve predictions for less frequent bigrams.\n",
        "*   The model's performance is limited by the size and specificity of the single Wikipedia page used for training. Training on a larger, more diverse corpus would likely yield better prediction results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "782df995"
      },
      "source": [
        "# Task\n",
        "Build a trigram language model with Add-One smoothing using the `wikipedia` library and train it on a diverse corpus to predict the next word or phrase."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1918bbe8"
      },
      "source": [
        "## Load and preprocess data\n",
        "\n",
        "### Subtask:\n",
        "Load text data from a more diverse corpus (e.g., multiple Wikipedia pages, or other text sources) and preprocess it by converting to lowercase, removing punctuation, and tokenizing into words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9cd409ae"
      },
      "source": [
        "**Reasoning**:\n",
        "Load text data from multiple Wikipedia pages, preprocess it by converting to lowercase, removing punctuation, and tokenizing into words as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be4493af",
        "outputId": "5ed0e8c3-d6cb-46e7-b196-e54a35fd68ff"
      },
      "source": [
        "import wikipedia\n",
        "import string\n",
        "\n",
        "# Create a list of Wikipedia page titles for a diverse corpus\n",
        "page_titles = [\"History\", \"Science\", \"Art\", \"Technology\", \"Literature\", \"Philosophy\"]\n",
        "combined_content = \"\"\n",
        "\n",
        "# Iterate through the list of page titles and get content\n",
        "for page_title in page_titles:\n",
        "    print(f\"Processing page: {page_title}\")\n",
        "    try:\n",
        "        page_content = wikipedia.page(page_title).content\n",
        "        combined_content += page_content\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(f\"Page '{page_title}' not found.\")\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        print(f\"Disambiguation page for '{page_title}'. Options: {e.options}\")\n",
        "        try:\n",
        "            page_content = wikipedia.page(e.options[0]).content\n",
        "            combined_content += page_content\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            print(f\"Could not get content from the first option for '{page_title}'.\")\n",
        "\n",
        "# Convert to lowercase\n",
        "lowercase_text = combined_content.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "cleaned_text = lowercase_text.translate(translator)\n",
        "\n",
        "# Tokenize into words\n",
        "words = cleaned_text.split()\n",
        "\n",
        "print(f\"\\nFirst 20 tokens from the diverse corpus: {words[:20]}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing page: History\n",
            "Processing page: Science\n",
            "Disambiguation page for 'Science'. Options: ['Sincé', 'Sińce', 'Since (film)', 'Since (album)', 'Since (rapper)', 'Snice']\n",
            "Processing page: Art\n",
            "Processing page: Technology\n",
            "Processing page: Literature\n",
            "Processing page: Philosophy\n",
            "\n",
            "First 20 tokens from the diverse corpus: ['history', 'is', 'the', 'systematic', 'study', 'of', 'the', 'past', 'focusing', 'primarily', 'on', 'the', 'human', 'past', 'as', 'an', 'academic', 'discipline', 'it', 'analyses']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ed3e70e6"
      },
      "source": [
        "## Create trigrams\n",
        "\n",
        "### Subtask:\n",
        "Generate trigrams (sequences of three consecutive words) from the tokenized text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "726a27ea"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate trigrams from the tokenized words list and print the first 10 trigrams.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28b2eb24",
        "outputId": "e99ee4e3-9c71-44f5-d72d-fa3d6b12de1b"
      },
      "source": [
        "trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    trigram = (words[i], words[i+1], words[i+2])\n",
        "    trigrams.append(trigram)\n",
        "\n",
        "print(\"First 10 trigrams:\")\n",
        "for trigram in trigrams[:10]:\n",
        "    print(trigram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 trigrams:\n",
            "('history', 'is', 'the')\n",
            "('is', 'the', 'systematic')\n",
            "('the', 'systematic', 'study')\n",
            "('systematic', 'study', 'of')\n",
            "('study', 'of', 'the')\n",
            "('of', 'the', 'past')\n",
            "('the', 'past', 'focusing')\n",
            "('past', 'focusing', 'primarily')\n",
            "('focusing', 'primarily', 'on')\n",
            "('primarily', 'on', 'the')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2d2b0b87"
      },
      "source": [
        "## Build trigram frequency table\n",
        "\n",
        "### Subtask:\n",
        "Create a frequency distribution of the trigrams to count how many times each trigram appears.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b4545f70"
      },
      "source": [
        "**Reasoning**:\n",
        "Import Counter and build the trigram frequency table using the list of trigrams, then print the 10 most common trigrams.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "140b23b1",
        "outputId": "5f26d929-6c7f-43f5-9a0a-68173279d0d3"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "trigram_frequencies = Counter(trigrams)\n",
        "\n",
        "print(\"10 most common trigrams:\")\n",
        "for trigram, count in trigram_frequencies.most_common(10):\n",
        "    print(f\"{trigram}: {count}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 most common trigrams:\n",
            "('the', 'study', 'of'): 21\n",
            "('the', 'history', 'of'): 18\n",
            "('as', 'well', 'as'): 18\n",
            "('such', 'as', 'the'): 17\n",
            "('the', 'philosophy', 'of'): 16\n",
            "('the', 'invention', 'of'): 15\n",
            "('in', 'the', 'form'): 12\n",
            "('the', 'form', 'of'): 12\n",
            "('one', 'of', 'the'): 12\n",
            "('the', 'emergence', 'of'): 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be4b7c72"
      },
      "source": [
        "## Build next word prediction function\n",
        "\n",
        "### Subtask:\n",
        "Create a function that takes two words as input and predicts the next word based on the trigram frequencies, incorporating Add-One smoothing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8b731e4e"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the function to predict the next word based on trigram frequencies with Add-One smoothing, calculate the vocabulary size, and test the function with example word pairs.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "613b86f1",
        "outputId": "97650049-bde4-4921-a2ba-8810a28f4b17"
      },
      "source": [
        "def predict_next_word_smoothed(word1, word2, trigram_frequencies, vocabulary_size):\n",
        "    \"\"\"\n",
        "    Predicts the next word based on trigram frequencies with Add-One smoothing.\n",
        "\n",
        "    Args:\n",
        "        word1: The first word of the bigram.\n",
        "        word2: The second word of the bigram.\n",
        "        trigram_frequencies: A Counter object containing trigram frequencies.\n",
        "        vocabulary_size: The total number of unique words in the corpus.\n",
        "\n",
        "    Returns:\n",
        "        The most frequent third word that follows the bigram after smoothing,\n",
        "        or None if no matching trigrams are found and the vocabulary is empty.\n",
        "    \"\"\"\n",
        "    matching_trigrams = {}\n",
        "    for trigram, frequency in trigram_frequencies.items():\n",
        "        if trigram[0] == word1 and trigram[1] == word2:\n",
        "            matching_trigrams[trigram[2]] = frequency\n",
        "\n",
        "    # Apply Add-One smoothing\n",
        "    smoothed_frequencies = {}\n",
        "    unique_words = set(words) # Get the vocabulary from the global 'words' list\n",
        "\n",
        "    for word in unique_words:\n",
        "        smoothed_frequencies[word] = matching_trigrams.get(word, 0) + 1\n",
        "\n",
        "    if smoothed_frequencies:\n",
        "        # Find the word with the highest smoothed frequency\n",
        "        predicted_word = max(smoothed_frequencies, key=smoothed_frequencies.get)\n",
        "        return predicted_word\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Calculate the vocabulary size\n",
        "vocabulary_size = len(set(words))\n",
        "print(f\"Vocabulary size: {vocabulary_size}\")\n",
        "\n",
        "# Test the predict_next_word_smoothed function with some example word pairs\n",
        "test_cases = [\n",
        "    (\"the\", \"study\"),\n",
        "    (\"history\", \"is\"),\n",
        "    (\"as\", \"well\"),\n",
        "    (\"apple\", \"banana\") # Example of words not expected to be found\n",
        "]\n",
        "\n",
        "for word1, word2 in test_cases:\n",
        "    prediction = predict_next_word_smoothed(word1, word2, trigram_frequencies, vocabulary_size)\n",
        "    print(f\"Given '{word1} {word2}', the predicted next word is: {prediction}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 4977\n",
            "Given 'the study', the predicted next word is: of\n",
            "Given 'history is', the predicted next word is: a\n",
            "Given 'as well', the predicted next word is: as\n",
            "Given 'apple banana', the predicted next word is: coloured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "db3c4c20"
      },
      "source": [
        "## Test the model\n",
        "\n",
        "### Subtask:\n",
        "Test the prediction function with some example word pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "122463bb"
      },
      "source": [
        "**Reasoning**:\n",
        "Call the predict_next_word_smoothed function with different word pairs and print the results to test its functionality.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7bf46437",
        "outputId": "8ad7d519-a9c3-47f0-e873-29e3cdffad9e"
      },
      "source": [
        "# Test the prediction function with some example word pairs\n",
        "test_cases = [\n",
        "    (\"the\", \"study\"),\n",
        "    (\"history\", \"is\"),\n",
        "    (\"as\", \"well\"),\n",
        "    (\"art\", \"history\"),\n",
        "    (\"technology\", \"and\"),\n",
        "    (\"nonexistent\", \"bigram\") # Example of words not expected to be found\n",
        "]\n",
        "\n",
        "for word1, word2 in test_cases:\n",
        "    prediction = predict_next_word_smoothed(word1, word2, trigram_frequencies, vocabulary_size)\n",
        "    print(f\"Given '{word1} {word2}', the predicted next word is: {prediction}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Given 'the study', the predicted next word is: of\n",
            "Given 'history is', the predicted next word is: a\n",
            "Given 'as well', the predicted next word is: as\n",
            "Given 'art history', the predicted next word is: another\n",
            "Given 'technology and', the predicted next word is: nature\n",
            "Given 'nonexistent bigram', the predicted next word is: coloured\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "599e1c88"
      },
      "source": [
        "## Evaluate and refine\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the model's performance on the diverse corpus and explore ways to further refine the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f2268b58"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to evaluate the model's performance, identify limitations, and explore refinements. I will evaluate by calculating perplexity on a held-out test set and qualitatively assessing predictions. This will help identify limitations, such as the impact of smoothing and corpus dependence. Finally, I will discuss potential refinements based on these findings. Since I don't have a separate test set, I will simulate one by taking a small portion of the existing data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09b519af",
        "outputId": "75f238a8-c12e-4357-ec0f-99a3f72147d5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Split data for evaluation (simulate a test set)\n",
        "# We'll use the last 100 bigrams for testing\n",
        "if len(words) > 200: # Ensure enough data for a test set\n",
        "    test_words = words[-200:]\n",
        "    training_words = words[:-200]\n",
        "else:\n",
        "    test_words = words\n",
        "    training_words = words\n",
        "\n",
        "# Rebuild trigrams and frequencies on the training data\n",
        "training_trigrams = []\n",
        "for i in range(len(training_words) - 2):\n",
        "    trigram = (training_words[i], training_words[i+1], training_words[i+2])\n",
        "    training_trigrams.append(trigram)\n",
        "\n",
        "training_trigram_frequencies = Counter(training_trigrams)\n",
        "training_vocabulary_size = len(set(training_words))\n",
        "\n",
        "# Calculate perplexity on the simulated test set\n",
        "# Perplexity = (1/N * sum(-log2(P(w_i | w_{i-2}, w_{i-1}))))\n",
        "# where N is the number of trigrams in the test set\n",
        "# and P is the smoothed probability from the model.\n",
        "\n",
        "log_prob_sum = 0\n",
        "n_test_trigrams = 0\n",
        "\n",
        "for i in range(len(test_words) - 2):\n",
        "    w1, w2, w3 = test_words[i], test_words[i+1], test_words[i+2]\n",
        "\n",
        "    # Calculate smoothed probability P(w3 | w1, w2)\n",
        "    # P(w3 | w1, w2) = (count(w1, w2, w3) + 1) / (count(w1, w2) + Vocabulary Size)\n",
        "    bigram_count = training_trigram_frequencies[(w1, w2, 'any_word')] -1 # Placeholder to count bigram occurrences\n",
        "    # Need to count occurrences of the bigram (w1, w2)\n",
        "    bigram_freq = sum(1 for trigram in training_trigrams if trigram[0] == w1 and trigram[1] == w2)\n",
        "\n",
        "    smoothed_prob = (training_trigram_frequencies[(w1, w2, w3)] + 1) / (bigram_freq + training_vocabulary_size)\n",
        "\n",
        "    if smoothed_prob > 0:\n",
        "        log_prob_sum += np.log2(smoothed_prob)\n",
        "        n_test_trigrams += 1\n",
        "    else:\n",
        "        # This case should theoretically not happen with Add-One smoothing\n",
        "        # but including for robustness.\n",
        "        print(f\"Warning: Zero probability for trigram: ({w1}, {w2}, {w3})\")\n",
        "\n",
        "\n",
        "if n_test_trigrams > 0:\n",
        "    average_log_prob = log_prob_sum / n_test_trigrams\n",
        "    perplexity = 2 ** (-average_log_prob)\n",
        "    print(f\"\\nPerplexity on simulated test set: {perplexity}\")\n",
        "else:\n",
        "    print(\"\\nNot enough test trigrams to calculate perplexity.\")\n",
        "\n",
        "# Qualitative assessment of predictions on the test set's bigrams\n",
        "print(\"\\nQualitative assessment of predictions on test set bigrams:\")\n",
        "for i in range(len(test_words) - 2):\n",
        "    w1, w2, w3_actual = test_words[i], test_words[i+1], test_words[i+2]\n",
        "    predicted_w3 = predict_next_word_smoothed(w1, w2, training_trigram_frequencies, training_vocabulary_size)\n",
        "    if predicted_w3:\n",
        "        print(f\"Bigram: '{w1} {w2}', Actual Next Word: '{w3_actual}', Predicted Next Word: '{predicted_w3}'\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Perplexity on simulated test set: 4461.6468134546785\n",
            "\n",
            "Qualitative assessment of predictions on test set bigrams:\n",
            "Bigram: 'gandhis philosophy', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'philosophy of', Actual Next Word: 'nonviolence', Predicted Next Word: 'religion'\n",
            "Bigram: 'of nonviolence', Actual Next Word: 'shaped', Predicted Next Word: 'coloured'\n",
            "Bigram: 'nonviolence shaped', Actual Next Word: 'the', Predicted Next Word: 'coloured'\n",
            "Bigram: 'shaped the', Actual Next Word: 'indian', Predicted Next Word: 'period'\n",
            "Bigram: 'the indian', Actual Next Word: 'independence', Predicted Next Word: 'coloured'\n",
            "Bigram: 'indian independence', Actual Next Word: 'movement', Predicted Next Word: 'coloured'\n",
            "Bigram: 'independence movement', Actual Next Word: 'an', Predicted Next Word: 'coloured'\n",
            "Bigram: 'movement an', Actual Next Word: 'example', Predicted Next Word: 'coloured'\n",
            "Bigram: 'an example', Actual Next Word: 'of', Predicted Next Word: 'is'\n",
            "Bigram: 'example of', Actual Next Word: 'the', Predicted Next Word: 'this'\n",
            "Bigram: 'of the', Actual Next Word: 'cultural', Predicted Next Word: 'past'\n",
            "Bigram: 'the cultural', Actual Next Word: 'and', Predicted Next Word: 'social'\n",
            "Bigram: 'cultural and', Actual Next Word: 'critical', Predicted Next Word: 'political'\n",
            "Bigram: 'and critical', Actual Next Word: 'role', Predicted Next Word: 'questioning'\n",
            "Bigram: 'critical role', Actual Next Word: 'of', Predicted Next Word: 'in'\n",
            "Bigram: 'role of', Actual Next Word: 'philosophy', Predicted Next Word: 'philosophical'\n",
            "Bigram: 'of philosophy', Actual Next Word: 'is', Predicted Next Word: 'is'\n",
            "Bigram: 'philosophy is', Actual Next Word: 'found', Predicted Next Word: 'the'\n",
            "Bigram: 'is found', Actual Next Word: 'in', Predicted Next Word: 'coloured'\n",
            "Bigram: 'found in', Actual Next Word: 'its', Predicted Next Word: 'the'\n",
            "Bigram: 'in its', Actual Next Word: 'influence', Predicted Next Word: 'definition'\n",
            "Bigram: 'its influence', Actual Next Word: 'on', Predicted Next Word: 'on'\n",
            "Bigram: 'influence on', Actual Next Word: 'the', Predicted Next Word: 'the'\n",
            "Bigram: 'on the', Actual Next Word: 'feminist', Predicted Next Word: 'idea'\n",
            "Bigram: 'the feminist', Actual Next Word: 'movement', Predicted Next Word: 'coloured'\n",
            "Bigram: 'feminist movement', Actual Next Word: 'through', Predicted Next Word: 'coloured'\n",
            "Bigram: 'movement through', Actual Next Word: 'philosophers', Predicted Next Word: 'coloured'\n",
            "Bigram: 'through philosophers', Actual Next Word: 'such', Predicted Next Word: 'coloured'\n",
            "Bigram: 'philosophers such', Actual Next Word: 'as', Predicted Next Word: 'as'\n",
            "Bigram: 'such as', Actual Next Word: 'mary', Predicted Next Word: 'the'\n",
            "Bigram: 'as mary', Actual Next Word: 'wollstonecraft', Predicted Next Word: 'coloured'\n",
            "Bigram: 'mary wollstonecraft', Actual Next Word: 'simone', Predicted Next Word: 'coloured'\n",
            "Bigram: 'wollstonecraft simone', Actual Next Word: 'de', Predicted Next Word: 'coloured'\n",
            "Bigram: 'simone de', Actual Next Word: 'beauvoir', Predicted Next Word: 'coloured'\n",
            "Bigram: 'de beauvoir', Actual Next Word: 'and', Predicted Next Word: 'coloured'\n",
            "Bigram: 'beauvoir and', Actual Next Word: 'judith', Predicted Next Word: 'coloured'\n",
            "Bigram: 'and judith', Actual Next Word: 'butler', Predicted Next Word: 'coloured'\n",
            "Bigram: 'judith butler', Actual Next Word: 'it', Predicted Next Word: 'coloured'\n",
            "Bigram: 'butler it', Actual Next Word: 'has', Predicted Next Word: 'coloured'\n",
            "Bigram: 'it has', Actual Next Word: 'shaped', Predicted Next Word: 'resulted'\n",
            "Bigram: 'has shaped', Actual Next Word: 'the', Predicted Next Word: 'coloured'\n",
            "Bigram: 'shaped the', Actual Next Word: 'understanding', Predicted Next Word: 'period'\n",
            "Bigram: 'the understanding', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'understanding of', Actual Next Word: 'key', Predicted Next Word: 'the'\n",
            "Bigram: 'of key', Actual Next Word: 'concepts', Predicted Next Word: 'coloured'\n",
            "Bigram: 'key concepts', Actual Next Word: 'in', Predicted Next Word: 'coloured'\n",
            "Bigram: 'concepts in', Actual Next Word: 'feminism', Predicted Next Word: 'coloured'\n",
            "Bigram: 'in feminism', Actual Next Word: 'for', Predicted Next Word: 'coloured'\n",
            "Bigram: 'feminism for', Actual Next Word: 'instance', Predicted Next Word: 'coloured'\n",
            "Bigram: 'for instance', Actual Next Word: 'the', Predicted Next Word: 'natural'\n",
            "Bigram: 'instance the', Actual Next Word: 'meaning', Predicted Next Word: 'discovery'\n",
            "Bigram: 'the meaning', Actual Next Word: 'of', Predicted Next Word: 'of'\n",
            "Bigram: 'meaning of', Actual Next Word: 'gender', Predicted Next Word: 'which'\n",
            "Bigram: 'of gender', Actual Next Word: 'how', Predicted Next Word: 'in'\n",
            "Bigram: 'gender how', Actual Next Word: 'it', Predicted Next Word: 'coloured'\n",
            "Bigram: 'how it', Actual Next Word: 'differs', Predicted Next Word: 'arises'\n",
            "Bigram: 'it differs', Actual Next Word: 'from', Predicted Next Word: 'from'\n",
            "Bigram: 'differs from', Actual Next Word: 'biological', Predicted Next Word: 'theology'\n",
            "Bigram: 'from biological', Actual Next Word: 'sex', Predicted Next Word: 'coloured'\n",
            "Bigram: 'biological sex', Actual Next Word: 'and', Predicted Next Word: 'coloured'\n",
            "Bigram: 'sex and', Actual Next Word: 'what', Predicted Next Word: 'coloured'\n",
            "Bigram: 'and what', Actual Next Word: 'role', Predicted Next Word: 'is'\n",
            "Bigram: 'what role', Actual Next Word: 'it', Predicted Next Word: 'coloured'\n",
            "Bigram: 'role it', Actual Next Word: 'plays', Predicted Next Word: 'coloured'\n",
            "Bigram: 'it plays', Actual Next Word: 'in', Predicted Next Word: 'a'\n",
            "Bigram: 'plays in', Actual Next Word: 'the', Predicted Next Word: 'coloured'\n",
            "Bigram: 'in the', Actual Next Word: 'formation', Predicted Next Word: 'form'\n",
            "Bigram: 'the formation', Actual Next Word: 'of', Predicted Next Word: 'of'\n",
            "Bigram: 'formation of', Actual Next Word: 'personal', Predicted Next Word: 'powerful'\n",
            "Bigram: 'of personal', Actual Next Word: 'identity', Predicted Next Word: 'values'\n",
            "Bigram: 'personal identity', Actual Next Word: 'philosophers', Predicted Next Word: 'coloured'\n",
            "Bigram: 'identity philosophers', Actual Next Word: 'have', Predicted Next Word: 'coloured'\n",
            "Bigram: 'philosophers have', Actual Next Word: 'also', Predicted Next Word: 'suggested'\n",
            "Bigram: 'have also', Actual Next Word: 'investigated', Predicted Next Word: 'been'\n",
            "Bigram: 'also investigated', Actual Next Word: 'the', Predicted Next Word: 'coloured'\n",
            "Bigram: 'investigated the', Actual Next Word: 'concepts', Predicted Next Word: 'coloured'\n",
            "Bigram: 'the concepts', Actual Next Word: 'of', Predicted Next Word: 'of'\n",
            "Bigram: 'concepts of', Actual Next Word: 'justice', Predicted Next Word: 'meaning'\n",
            "Bigram: 'of justice', Actual Next Word: 'and', Predicted Next Word: 'coloured'\n",
            "Bigram: 'justice and', Actual Next Word: 'equality', Predicted Next Word: 'coloured'\n",
            "Bigram: 'and equality', Actual Next Word: 'and', Predicted Next Word: 'coloured'\n",
            "Bigram: 'equality and', Actual Next Word: 'their', Predicted Next Word: 'coloured'\n",
            "Bigram: 'and their', Actual Next Word: 'implications', Predicted Next Word: 'existence'\n",
            "Bigram: 'their implications', Actual Next Word: 'with', Predicted Next Word: 'coloured'\n",
            "Bigram: 'implications with', Actual Next Word: 'respect', Predicted Next Word: 'coloured'\n",
            "Bigram: 'with respect', Actual Next Word: 'to', Predicted Next Word: 'coloured'\n",
            "Bigram: 'respect to', Actual Next Word: 'the', Predicted Next Word: 'coloured'\n",
            "Bigram: 'to the', Actual Next Word: 'prejudicial', Predicted Next Word: 'academic'\n",
            "Bigram: 'the prejudicial', Actual Next Word: 'treatment', Predicted Next Word: 'coloured'\n",
            "Bigram: 'prejudicial treatment', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'treatment of', Actual Next Word: 'women', Predicted Next Word: 'controversial'\n",
            "Bigram: 'of women', Actual Next Word: 'in', Predicted Next Word: 'to'\n",
            "Bigram: 'women in', Actual Next Word: 'maledominated', Predicted Next Word: 'coloured'\n",
            "Bigram: 'in maledominated', Actual Next Word: 'societies', Predicted Next Word: 'coloured'\n",
            "Bigram: 'maledominated societies', Actual Next Word: 'the', Predicted Next Word: 'coloured'\n",
            "Bigram: 'societies the', Actual Next Word: 'idea', Predicted Next Word: 'coloured'\n",
            "Bigram: 'the idea', Actual Next Word: 'that', Predicted Next Word: 'that'\n",
            "Bigram: 'idea that', Actual Next Word: 'philosophy', Predicted Next Word: 'an'\n",
            "Bigram: 'that philosophy', Actual Next Word: 'is', Predicted Next Word: 'is'\n",
            "Bigram: 'philosophy is', Actual Next Word: 'useful', Predicted Next Word: 'the'\n",
            "Bigram: 'is useful', Actual Next Word: 'for', Predicted Next Word: 'coloured'\n",
            "Bigram: 'useful for', Actual Next Word: 'many', Predicted Next Word: 'coloured'\n",
            "Bigram: 'for many', Actual Next Word: 'aspects', Predicted Next Word: 'coloured'\n",
            "Bigram: 'many aspects', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'aspects of', Actual Next Word: 'life', Predicted Next Word: 'experience'\n",
            "Bigram: 'of life', Actual Next Word: 'and', Predicted Next Word: 'and'\n",
            "Bigram: 'life and', Actual Next Word: 'society', Predicted Next Word: 'living'\n",
            "Bigram: 'and society', Actual Next Word: 'is', Predicted Next Word: 'coloured'\n",
            "Bigram: 'society is', Actual Next Word: 'sometimes', Predicted Next Word: 'coloured'\n",
            "Bigram: 'is sometimes', Actual Next Word: 'rejected', Predicted Next Word: 'understood'\n",
            "Bigram: 'sometimes rejected', Actual Next Word: 'according', Predicted Next Word: 'coloured'\n",
            "Bigram: 'rejected according', Actual Next Word: 'to', Predicted Next Word: 'coloured'\n",
            "Bigram: 'according to', Actual Next Word: 'one', Predicted Next Word: 'one'\n",
            "Bigram: 'to one', Actual Next Word: 'such', Predicted Next Word: 'view'\n",
            "Bigram: 'one such', Actual Next Word: 'view', Predicted Next Word: 'view'\n",
            "Bigram: 'such view', Actual Next Word: 'philosophy', Predicted Next Word: 'is'\n",
            "Bigram: 'view philosophy', Actual Next Word: 'is', Predicted Next Word: 'is'\n",
            "Bigram: 'philosophy is', Actual Next Word: 'mainly', Predicted Next Word: 'the'\n",
            "Bigram: 'is mainly', Actual Next Word: 'undertaken', Predicted Next Word: 'associated'\n",
            "Bigram: 'mainly undertaken', Actual Next Word: 'for', Predicted Next Word: 'coloured'\n",
            "Bigram: 'undertaken for', Actual Next Word: 'its', Predicted Next Word: 'coloured'\n",
            "Bigram: 'for its', Actual Next Word: 'own', Predicted Next Word: 'coloured'\n",
            "Bigram: 'its own', Actual Next Word: 'sake', Predicted Next Word: 'methods'\n",
            "Bigram: 'own sake', Actual Next Word: 'and', Predicted Next Word: 'coloured'\n",
            "Bigram: 'sake and', Actual Next Word: 'does', Predicted Next Word: 'coloured'\n",
            "Bigram: 'and does', Actual Next Word: 'not', Predicted Next Word: 'coloured'\n",
            "Bigram: 'does not', Actual Next Word: 'make', Predicted Next Word: 'practice'\n",
            "Bigram: 'not make', Actual Next Word: 'significant', Predicted Next Word: 'coloured'\n",
            "Bigram: 'make significant', Actual Next Word: 'contributions', Predicted Next Word: 'coloured'\n",
            "Bigram: 'significant contributions', Actual Next Word: 'to', Predicted Next Word: 'coloured'\n",
            "Bigram: 'contributions to', Actual Next Word: 'existing', Predicted Next Word: 'coloured'\n",
            "Bigram: 'to existing', Actual Next Word: 'practices', Predicted Next Word: 'coloured'\n",
            "Bigram: 'existing practices', Actual Next Word: 'or', Predicted Next Word: 'coloured'\n",
            "Bigram: 'practices or', Actual Next Word: 'external', Predicted Next Word: 'objects'\n",
            "Bigram: 'or external', Actual Next Word: 'goals', Predicted Next Word: 'coloured'\n",
            "Bigram: 'external goals', Actual Next Word: 'see', Predicted Next Word: 'coloured'\n",
            "Bigram: 'goals see', Actual Next Word: 'also', Predicted Next Word: 'coloured'\n",
            "Bigram: 'see also', Actual Next Word: 'references', Predicted Next Word: 'internet'\n",
            "Bigram: 'also references', Actual Next Word: 'notes', Predicted Next Word: 'citations'\n",
            "Bigram: 'references notes', Actual Next Word: 'citations', Predicted Next Word: 'citations'\n",
            "Bigram: 'notes citations', Actual Next Word: 'bibliography', Predicted Next Word: 'sources'\n",
            "Bigram: 'citations bibliography', Actual Next Word: 'external', Predicted Next Word: 'coloured'\n",
            "Bigram: 'bibliography external', Actual Next Word: 'links', Predicted Next Word: 'coloured'\n",
            "Bigram: 'external links', Actual Next Word: 'internet', Predicted Next Word: 'in'\n",
            "Bigram: 'links internet', Actual Next Word: 'encyclopedia', Predicted Next Word: 'history'\n",
            "Bigram: 'internet encyclopedia', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'encyclopedia of', Actual Next Word: 'philosophy', Predicted Next Word: 'coloured'\n",
            "Bigram: 'of philosophy', Actual Next Word: '–', Predicted Next Word: 'is'\n",
            "Bigram: 'philosophy –', Actual Next Word: 'a', Predicted Next Word: 'coloured'\n",
            "Bigram: '– a', Actual Next Word: 'peerreviewed', Predicted Next Word: 'coloured'\n",
            "Bigram: 'a peerreviewed', Actual Next Word: 'online', Predicted Next Word: 'coloured'\n",
            "Bigram: 'peerreviewed online', Actual Next Word: 'encyclopedia', Predicted Next Word: 'coloured'\n",
            "Bigram: 'online encyclopedia', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'encyclopedia of', Actual Next Word: 'philosophy', Predicted Next Word: 'coloured'\n",
            "Bigram: 'of philosophy', Actual Next Word: 'stanford', Predicted Next Word: 'is'\n",
            "Bigram: 'philosophy stanford', Actual Next Word: 'encyclopedia', Predicted Next Word: 'coloured'\n",
            "Bigram: 'stanford encyclopedia', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'encyclopedia of', Actual Next Word: 'philosophy', Predicted Next Word: 'coloured'\n",
            "Bigram: 'of philosophy', Actual Next Word: '–', Predicted Next Word: 'is'\n",
            "Bigram: 'philosophy –', Actual Next Word: 'an', Predicted Next Word: 'coloured'\n",
            "Bigram: '– an', Actual Next Word: 'online', Predicted Next Word: 'coloured'\n",
            "Bigram: 'an online', Actual Next Word: 'encyclopedia', Predicted Next Word: 'coloured'\n",
            "Bigram: 'online encyclopedia', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'encyclopedia of', Actual Next Word: 'philosophy', Predicted Next Word: 'coloured'\n",
            "Bigram: 'of philosophy', Actual Next Word: 'maintained', Predicted Next Word: 'is'\n",
            "Bigram: 'philosophy maintained', Actual Next Word: 'by', Predicted Next Word: 'coloured'\n",
            "Bigram: 'maintained by', Actual Next Word: 'stanford', Predicted Next Word: 'coloured'\n",
            "Bigram: 'by stanford', Actual Next Word: 'university', Predicted Next Word: 'coloured'\n",
            "Bigram: 'stanford university', Actual Next Word: 'philpapers', Predicted Next Word: 'coloured'\n",
            "Bigram: 'university philpapers', Actual Next Word: '–', Predicted Next Word: 'coloured'\n",
            "Bigram: 'philpapers –', Actual Next Word: 'a', Predicted Next Word: 'coloured'\n",
            "Bigram: '– a', Actual Next Word: 'comprehensive', Predicted Next Word: 'coloured'\n",
            "Bigram: 'a comprehensive', Actual Next Word: 'directory', Predicted Next Word: 'understanding'\n",
            "Bigram: 'comprehensive directory', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'directory of', Actual Next Word: 'online', Predicted Next Word: 'coloured'\n",
            "Bigram: 'of online', Actual Next Word: 'philosophical', Predicted Next Word: 'coloured'\n",
            "Bigram: 'online philosophical', Actual Next Word: 'articles', Predicted Next Word: 'coloured'\n",
            "Bigram: 'philosophical articles', Actual Next Word: 'and', Predicted Next Word: 'coloured'\n",
            "Bigram: 'articles and', Actual Next Word: 'books', Predicted Next Word: 'coloured'\n",
            "Bigram: 'and books', Actual Next Word: 'by', Predicted Next Word: 'coloured'\n",
            "Bigram: 'books by', Actual Next Word: 'academic', Predicted Next Word: 'coloured'\n",
            "Bigram: 'by academic', Actual Next Word: 'philosophers', Predicted Next Word: 'coloured'\n",
            "Bigram: 'academic philosophers', Actual Next Word: 'internet', Predicted Next Word: 'coloured'\n",
            "Bigram: 'philosophers internet', Actual Next Word: 'philosophy', Predicted Next Word: 'coloured'\n",
            "Bigram: 'internet philosophy', Actual Next Word: 'ontology', Predicted Next Word: 'coloured'\n",
            "Bigram: 'philosophy ontology', Actual Next Word: 'project', Predicted Next Word: 'coloured'\n",
            "Bigram: 'ontology project', Actual Next Word: '–', Predicted Next Word: 'coloured'\n",
            "Bigram: 'project –', Actual Next Word: 'a', Predicted Next Word: 'coloured'\n",
            "Bigram: '– a', Actual Next Word: 'model', Predicted Next Word: 'coloured'\n",
            "Bigram: 'a model', Actual Next Word: 'of', Predicted Next Word: 'coloured'\n",
            "Bigram: 'model of', Actual Next Word: 'relationships', Predicted Next Word: 'coloured'\n",
            "Bigram: 'of relationships', Actual Next Word: 'between', Predicted Next Word: 'coloured'\n",
            "Bigram: 'relationships between', Actual Next Word: 'philosophical', Predicted Next Word: 'genres'\n",
            "Bigram: 'between philosophical', Actual Next Word: 'ideas', Predicted Next Word: 'coloured'\n",
            "Bigram: 'philosophical ideas', Actual Next Word: 'thinkers', Predicted Next Word: 'have'\n",
            "Bigram: 'ideas thinkers', Actual Next Word: 'and', Predicted Next Word: 'coloured'\n",
            "Bigram: 'thinkers and', Actual Next Word: 'journals', Predicted Next Word: 'coloured'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cf957b8a"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The combined corpus from the selected Wikipedia pages resulted in 4977 unique words.\n",
        "*   The most common trigrams in the combined corpus include \"the study of\", \"the history of\", and \"as well as\".\n",
        "*   The calculated perplexity for the trigram model with Add-One smoothing on the simulated test set was high at 4461.65, indicating poor predictive performance on this dataset.\n",
        "*   Qualitative testing showed that the model frequently predicted the word \"coloured\" regardless of the preceding bigram.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The model's high perplexity and the frequent prediction of a single word (\"coloured\") strongly suggest that the current corpus size is insufficient and significantly impacts the model's ability to learn meaningful patterns.\n",
        "*   Future work should focus on obtaining a much larger and more diverse text corpus and exploring more sophisticated smoothing techniques (like Kneser-Ney) to address the data sparsity issues inherent in n-gram models.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbad5679"
      },
      "source": [
        "# Task\n",
        "Build a trigram model for next word prediction using the `wikipedia` and `gutenberg` libraries for a diverse corpus, applying Kneser-Ney smoothing to address data sparsity issues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e321ca3"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary libraries using pip.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "347ca1ee"
      },
      "source": [
        "**Reasoning**:\n",
        "Attempt to install the `gutenberg` library again, this time in a separate cell, which sometimes helps with dependency resolution in environments like Colab. If it still fails, this will confirm the dependency issue.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d0eae3b7",
        "outputId": "0a91fe8a-02bd-44e3-e781-847a285436ad"
      },
      "source": [
        "%pip install gutenberg"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gutenberg\n",
            "  Using cached Gutenberg-0.8.2-py3-none-any.whl.metadata (9.4 kB)\n",
            "\u001b[33mWARNING: Package 'Gutenberg' has an invalid Requires-Python: Invalid specifier: '>=2.7.*'\u001b[0m\u001b[33m\n",
            "\u001b[0mCollecting SPARQLWrapper>=1.8.2 (from gutenberg)\n",
            "  Using cached SPARQLWrapper-2.0.0-py3-none-any.whl.metadata (2.0 kB)\n",
            "Collecting bsddb3>=6.1.0 (from gutenberg)\n",
            "  Using cached bsddb3-6.2.9.tar.gz (230 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20c3d588"
      },
      "source": [
        "**Reasoning**:\n",
        "The installation of `gutenberg` failed again due to the `bsddb3` dependency. I will try to install `bsddb3` separately to see if that resolves the issue or provides a more specific error message for the dependency problem.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4afa3cae",
        "outputId": "73c2ce71-11b4-484b-c170-81fd5af2ed65"
      },
      "source": [
        "%pip install bsddb3"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting bsddb3\n",
            "  Using cached bsddb3-6.2.9.tar.gz (230 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "971b53bc"
      },
      "source": [
        "## Install necessary libraries\n",
        "\n",
        "### Subtask:\n",
        "Install the necessary libraries using pip.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340f7221"
      },
      "source": [
        "## Load and preprocess data\n",
        "\n",
        "### Subtask:\n",
        "Load text data from a diverse corpus using the `wikipedia` and `gutenberg` libraries, and preprocess it by converting to lowercase, removing punctuation, and tokenizing into words.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2ec74eae"
      },
      "source": [
        "**Reasoning**:\n",
        "Load text data from multiple Wikipedia pages and preprocess it by converting to lowercase, removing punctuation, and tokenizing into words. Since the previous attempts to install the `gutenberg` library failed, I will proceed without it, focusing only on the Wikipedia data as per the instructions, acknowledging the inability to use Gutenberg.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f6eebbe0",
        "outputId": "ec0e5465-bcd3-4449-9188-32b7eb4132fa"
      },
      "source": [
        "import wikipedia\n",
        "import string\n",
        "\n",
        "# Create a list of Wikipedia page titles for a diverse corpus\n",
        "page_titles = [\"History\", \"Science\", \"Art\", \"Technology\", \"Literature\", \"Philosophy\", \"Music\", \"Sports\", \"Politics\"]\n",
        "combined_content = \"\"\n",
        "\n",
        "# Iterate through the list of page titles and get content\n",
        "print(\"Loading data from Wikipedia...\")\n",
        "for page_title in page_titles:\n",
        "    print(f\"Processing page: {page_title}\")\n",
        "    try:\n",
        "        page_content = wikipedia.page(page_title).content\n",
        "        combined_content += page_content\n",
        "    except wikipedia.exceptions.PageError:\n",
        "        print(f\"Page '{page_title}' not found.\")\n",
        "    except wikipedia.exceptions.DisambiguationError as e:\n",
        "        print(f\"Disambiguation page for '{page_title}'. Options: {e.options}\")\n",
        "        try:\n",
        "            # Attempt to get content from the first option\n",
        "            page_content = wikipedia.page(e.options[0]).content\n",
        "            combined_content += page_content\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            print(f\"Could not get content from the first option for '{page_title}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"An unexpected error occurred for page '{page_title}': {e}\")\n",
        "\n",
        "# Convert to lowercase\n",
        "lowercase_text = combined_content.lower()\n",
        "\n",
        "# Remove punctuation\n",
        "translator = str.maketrans('', '', string.punctuation)\n",
        "cleaned_text = lowercase_text.translate(translator)\n",
        "\n",
        "# Tokenize into words\n",
        "words = cleaned_text.split()\n",
        "\n",
        "print(f\"\\nSuccessfully loaded and preprocessed data from Wikipedia.\")\n",
        "print(f\"Total number of words: {len(words)}\")\n",
        "print(f\"First 20 tokens from the diverse corpus: {words[:20]}\")\n",
        "\n",
        "# Since gutenberg installation failed in previous attempts, we will proceed without it.\n",
        "print(\"\\nNote: The gutenberg library could not be installed in previous steps, so only Wikipedia data is used.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading data from Wikipedia...\n",
            "Processing page: History\n",
            "Processing page: Science\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py:389: GuessedAtParserWarning: No parser was explicitly specified, so I'm using the best available HTML parser for this system (\"lxml\"). This usually isn't a problem, but if you run this code on another system, or in a different virtual environment, it may use a different parser and behave differently.\n",
            "\n",
            "The code that caused this warning is on line 389 of the file /usr/local/lib/python3.12/dist-packages/wikipedia/wikipedia.py. To get rid of this warning, pass the additional argument 'features=\"lxml\"' to the BeautifulSoup constructor.\n",
            "\n",
            "  lis = BeautifulSoup(html).find_all('li')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Disambiguation page for 'Science'. Options: ['Sincé', 'Sińce', 'Since (film)', 'Since (album)', 'Since (rapper)', 'Snice']\n",
            "Processing page: Art\n",
            "Processing page: Technology\n",
            "Processing page: Literature\n",
            "Processing page: Philosophy\n",
            "Processing page: Music\n",
            "Page 'Music' not found.\n",
            "Processing page: Sports\n",
            "Processing page: Politics\n",
            "Page 'Politics' not found.\n",
            "\n",
            "Successfully loaded and preprocessed data from Wikipedia.\n",
            "Total number of words: 31393\n",
            "First 20 tokens from the diverse corpus: ['history', 'is', 'the', 'systematic', 'study', 'of', 'the', 'past', 'focusing', 'primarily', 'on', 'the', 'human', 'past', 'as', 'an', 'academic', 'discipline', 'it', 'analyses']\n",
            "\n",
            "Note: The gutenberg library could not be installed in previous steps, so only Wikipedia data is used.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2edc722c"
      },
      "source": [
        "## Create trigrams\n",
        "\n",
        "### Subtask:\n",
        "Generate trigrams (sequences of three consecutive words) from the tokenized text.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b97a47e"
      },
      "source": [
        "**Reasoning**:\n",
        "Generate trigrams from the tokenized words list and print the first 10 trigrams as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85cf94ce",
        "outputId": "274150bc-e76b-4c5a-803d-5df186ae4790"
      },
      "source": [
        "trigrams = []\n",
        "for i in range(len(words) - 2):\n",
        "    trigram = (words[i], words[i+1], words[i+2])\n",
        "    trigrams.append(trigram)\n",
        "\n",
        "print(\"First 10 trigrams:\")\n",
        "for trigram in trigrams[:10]:\n",
        "    print(trigram)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First 10 trigrams:\n",
            "('history', 'is', 'the')\n",
            "('is', 'the', 'systematic')\n",
            "('the', 'systematic', 'study')\n",
            "('systematic', 'study', 'of')\n",
            "('study', 'of', 'the')\n",
            "('of', 'the', 'past')\n",
            "('the', 'past', 'focusing')\n",
            "('past', 'focusing', 'primarily')\n",
            "('focusing', 'primarily', 'on')\n",
            "('primarily', 'on', 'the')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f25a4ac2"
      },
      "source": [
        "## Build trigram frequency table\n",
        "\n",
        "### Subtask:\n",
        "Create a frequency distribution of the trigrams to count how many times each trigram appears.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a6b09f4"
      },
      "source": [
        "**Reasoning**:\n",
        "Import Counter and build the trigram frequency table using the list of trigrams, then print the 10 most common trigrams as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7d205522",
        "outputId": "b3b35a3c-e4e2-4b58-da08-78e3e419b78c"
      },
      "source": [
        "from collections import Counter\n",
        "\n",
        "trigram_frequencies = Counter(trigrams)\n",
        "\n",
        "print(\"10 most common trigrams:\")\n",
        "for trigram, count in trigram_frequencies.most_common(10):\n",
        "    print(f\"{trigram}: {count}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10 most common trigrams:\n",
            "('as', 'well', 'as'): 22\n",
            "('the', 'study', 'of'): 21\n",
            "('such', 'as', 'the'): 20\n",
            "('the', 'history', 'of'): 19\n",
            "('the', 'philosophy', 'of'): 17\n",
            "('the', 'invention', 'of'): 15\n",
            "('the', 'use', 'of'): 14\n",
            "('the', 'development', 'of'): 13\n",
            "('in', 'the', 'form'): 12\n",
            "('the', 'form', 'of'): 12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "771e93d5"
      },
      "source": [
        "## Build next word prediction function\n",
        "\n",
        "### Subtask:\n",
        "Create a function that takes two words as input and predicts the next word based on the trigram frequencies, incorporating Kneser-Ney smoothing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0a988123"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the function to predict the next word using Kneser-Ney smoothing based on trigram frequencies, calculate the vocabulary size, and test the function with example word pairs as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "888eaac9",
        "outputId": "e1e32a87-c1df-4767-a608-ee33135b5282"
      },
      "source": [
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "def predict_next_word_kneser_ney(word1, word2, trigram_frequencies, words):\n",
        "    \"\"\"\n",
        "    Predicts the next word based on trigram frequencies with Kneser-Ney smoothing.\n",
        "\n",
        "    Args:\n",
        "        word1: The first word of the bigram.\n",
        "        word2: The second word of the bigram.\n",
        "        trigram_frequencies: A Counter object containing trigram frequencies.\n",
        "        words: The full list of words in the corpus for vocabulary and context counts.\n",
        "\n",
        "    Returns:\n",
        "        The most likely next word based on Kneser-Ney smoothing, or None if no\n",
        "        matching bigrams are found and the vocabulary is empty.\n",
        "    \"\"\"\n",
        "    vocabulary = set(words)\n",
        "    vocabulary_size = len(vocabulary)\n",
        "\n",
        "    # Calculate count(w1, w2) - occurrences of the bigram\n",
        "    bigram_freq = sum(1 for trigram in trigram_frequencies if trigram[0] == word1 and trigram[1] == word2)\n",
        "\n",
        "    # Calculate count(w1, w2, w3) for all w3 following (w1, w2)\n",
        "    matching_trigrams = {trigram[2]: freq for trigram, freq in trigram_frequencies.items() if trigram[0] == word1 and trigram[1] == word2}\n",
        "\n",
        "    # Calculate the number of unique words that follow w2 (for the continuation probability)\n",
        "    words_following_w2 = set(trigram[2] for trigram in trigram_frequencies if trigram[1] == word2)\n",
        "    count_unique_following_w2 = len(words_following_w2)\n",
        "\n",
        "    # Calculate the total number of unique bigrams in the corpus\n",
        "    unique_bigrams = set((trigram[0], trigram[1]) for trigram in trigram_frequencies)\n",
        "    total_unique_bigrams = len(unique_bigrams)\n",
        "\n",
        "    # Discount factor (simplified for this example, can be learned from data)\n",
        "    discount = 0.75\n",
        "\n",
        "    smoothed_probabilities = {}\n",
        "\n",
        "    for w3 in vocabulary:\n",
        "        # Count(w1, w2, w3)\n",
        "        trigram_count = trigram_frequencies.get((word1, word2, w3), 0)\n",
        "\n",
        "        # Calculate the first part of the Kneser-Ney formula (discounted probability)\n",
        "        if bigram_freq > 0:\n",
        "            discounted_prob = max(0, trigram_count - discount) / bigram_freq\n",
        "        else:\n",
        "            discounted_prob = 0 # Cannot calculate discounted prob if bigram never occurred\n",
        "\n",
        "        # Calculate the continuation probability P_cont(w3)\n",
        "        # P_cont(w3) is proportional to the number of unique bigrams ending with w3\n",
        "        count_bigrams_ending_with_w3 = len(set((trigram[0], trigram[1]) for trigram in trigram_frequencies if trigram[2] == w3))\n",
        "\n",
        "        if total_unique_bigrams > 0:\n",
        "             p_cont_w3 = count_bigrams_ending_with_w3 / total_unique_bigrams\n",
        "        else:\n",
        "             p_cont_w3 = 0\n",
        "\n",
        "\n",
        "        # Calculate the interpolation weight (lambda)\n",
        "        # lambda(w1, w2) = (discount / count(w1, w2)) * |{w3: count(w1, w2, w3) > 0}|\n",
        "        # simplified: (discount / count(w1, w2)) * number of unique words that follow (w1, w2)\n",
        "        num_unique_followers = len(matching_trigrams)\n",
        "\n",
        "        if bigram_freq > 0:\n",
        "            lambda_w1_w2 = (discount / bigram_freq) * num_unique_followers\n",
        "        else:\n",
        "            # If bigram_freq is 0, the lambda term is 1, and the probability is solely based on continuation\n",
        "             lambda_w1_w2 = 1\n",
        "\n",
        "\n",
        "        # Combine the two parts using the interpolation weight\n",
        "        smoothed_prob = discounted_prob + lambda_w1_w2 * p_cont_w3\n",
        "\n",
        "        smoothed_probabilities[w3] = smoothed_prob\n",
        "\n",
        "    if smoothed_probabilities:\n",
        "        # Find the word with the highest smoothed probability\n",
        "        predicted_word = max(smoothed_probabilities, key=smoothed_probabilities.get)\n",
        "        return predicted_word\n",
        "    else:\n",
        "        return None # Should not happen if vocabulary is not empty\n",
        "\n",
        "# Calculate the vocabulary size\n",
        "# vocabulary_size = len(set(words)) # Already calculated inside the function\n",
        "\n",
        "# Test the predict_next_word_kneser_ney function with some example word pairs\n",
        "print(f\"Vocabulary size: {len(set(words))}\")\n",
        "\n",
        "test_cases = [\n",
        "    (\"the\", \"study\"),\n",
        "    (\"history\", \"is\"),\n",
        "    (\"as\", \"well\"),\n",
        "    (\"art\", \"history\"),\n",
        "    (\"technology\", \"and\"),\n",
        "    (\"nonexistent\", \"bigram\"), # Example of words not expected to be found\n",
        "    (\"of\", \"the\") # Common bigram\n",
        "]\n",
        "\n",
        "for word1, word2 in test_cases:\n",
        "    prediction = predict_next_word_kneser_ney(word1, word2, trigram_frequencies, words)\n",
        "    print(f\"Given '{word1} {word2}', the predicted next word is: {prediction}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 5685\n",
            "Given 'the study', the predicted next word is: of\n",
            "Given 'history is', the predicted next word is: a\n",
            "Given 'as well', the predicted next word is: as\n",
            "Given 'art history', the predicted next word is: and\n",
            "Given 'technology and', the predicted next word is: a\n",
            "Given 'nonexistent bigram', the predicted next word is: the\n",
            "Given 'of the', the predicted next word is: the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29b4b02f"
      },
      "source": [
        "## Test the model\n",
        "\n",
        "### Subtask:\n",
        "Test the prediction function with some example word pairs.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eb4e15fa"
      },
      "source": [
        "**Reasoning**:\n",
        "Call the predict_next_word_kneser_ney function with different word pairs and print the results to test its functionality, fulfilling the subtask requirements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4c98b0f8",
        "outputId": "d0a0fdec-6a66-460f-fd6a-6d38ecb5635f"
      },
      "source": [
        "# Test the predict_next_word_kneser_ney function with some example word pairs\n",
        "test_cases = [\n",
        "    (\"the\", \"study\"),\n",
        "    (\"history\", \"is\"),\n",
        "    (\"as\", \"well\"),\n",
        "    (\"art\", \"history\"),\n",
        "    (\"technology\", \"and\"),\n",
        "    (\"nonexistent\", \"bigram\"), # Example of words not expected to be found\n",
        "    (\"of\", \"the\"), # Common bigram\n",
        "    (\"in\", \"the\"), # Another common bigram\n",
        "    (\"science\", \"and\"), # Bigram related to one of the corpus topics\n",
        "    (\"literature\", \"is\") # Bigram related to another corpus topic\n",
        "]\n",
        "\n",
        "print(\"Testing predict_next_word_kneser_ney function:\")\n",
        "for word1, word2 in test_cases:\n",
        "    prediction = predict_next_word_kneser_ney(word1, word2, trigram_frequencies, words)\n",
        "    print(f\"Given '{word1} {word2}', the predicted next word is: {prediction}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing predict_next_word_kneser_ney function:\n",
            "Given 'the study', the predicted next word is: of\n",
            "Given 'history is', the predicted next word is: a\n",
            "Given 'as well', the predicted next word is: as\n",
            "Given 'art history', the predicted next word is: and\n",
            "Given 'technology and', the predicted next word is: a\n",
            "Given 'nonexistent bigram', the predicted next word is: the\n",
            "Given 'of the', the predicted next word is: the\n",
            "Given 'in the', the predicted next word is: form\n",
            "Given 'science and', the predicted next word is: the\n",
            "Given 'literature is', the predicted next word is: a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d540e993"
      },
      "source": [
        "## Evaluate and refine\n",
        "\n",
        "### Subtask:\n",
        "Evaluate the model's performance on the diverse corpus and explore ways to further refine the model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7139a3df"
      },
      "source": [
        "**Reasoning**:\n",
        "Split the data, re-calculate frequencies on the training set, implement perplexity calculation with Kneser-Ney smoothing, calculate perplexity, and perform qualitative assessment, then discuss findings as per the instructions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8074aa2e",
        "outputId": "399be990-f539-4585-9e5a-f0d19b81d02b"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# 1. Split the available words data into a training set and a small test set\n",
        "split_ratio = 0.8\n",
        "split_index = int(len(words) * split_ratio)\n",
        "training_words = words[:split_index]\n",
        "test_words = words[split_index:]\n",
        "\n",
        "print(f\"Training set size: {len(training_words)} words\")\n",
        "print(f\"Test set size: {len(test_words)} words\")\n",
        "\n",
        "# 2. Re-calculate the trigram_frequencies and the vocabulary based *only* on the training set.\n",
        "training_trigrams = []\n",
        "for i in range(len(training_words) - 2):\n",
        "    trigram = (training_words[i], training_words[i+1], training_words[i+2])\n",
        "    training_trigrams.append(trigram)\n",
        "\n",
        "training_trigram_frequencies = Counter(training_trigrams)\n",
        "training_vocabulary = set(training_words)\n",
        "training_vocabulary_size = len(training_vocabulary)\n",
        "\n",
        "print(f\"Training vocabulary size: {training_vocabulary_size}\")\n",
        "\n",
        "# Helper function to calculate Kneser-Ney smoothed probability P(w3 | w1, w2)\n",
        "def get_kneser_ney_prob(w1, w2, w3, training_trigram_frequencies, training_words):\n",
        "    \"\"\"\n",
        "    Calculates the Kneser-Ney smoothed probability P(w3 | w1, w2).\n",
        "\n",
        "    Args:\n",
        "        w1: The first word of the bigram.\n",
        "        w2: The second word of the bigram.\n",
        "        w3: The potential next word.\n",
        "        training_trigram_frequencies: Counter of trigram frequencies from training data.\n",
        "        training_words: List of words in the training corpus.\n",
        "\n",
        "    Returns:\n",
        "        The Kneser-Ney smoothed probability P(w3 | w1, w2).\n",
        "    \"\"\"\n",
        "    training_vocabulary = set(training_words)\n",
        "    training_vocabulary_size = len(training_vocabulary)\n",
        "    discount = 0.75 # Using a fixed discount for simplicity\n",
        "\n",
        "    # Count(w1, w2, w3)\n",
        "    trigram_count = training_trigram_frequencies.get((w1, w2, w3), 0)\n",
        "\n",
        "    # Count(w1, w2) - occurrences of the bigram in training data\n",
        "    # We can approximate this by summing frequencies of all trigrams starting with (w1, w2)\n",
        "    bigram_freq = sum(freq for (t1, t2, t3), freq in training_trigram_frequencies.items() if t1 == w1 and t2 == w2)\n",
        "\n",
        "    # Number of unique words that follow (w1, w2) in training data\n",
        "    num_unique_followers = len(set(t3 for (t1, t2, t3) in training_trigram_frequencies if t1 == w1 and t2 == w2))\n",
        "\n",
        "    # Calculate the first part of the Kneser-Ney formula (discounted probability)\n",
        "    if bigram_freq > 0:\n",
        "        discounted_prob = max(0, trigram_count - discount) / bigram_freq\n",
        "    else:\n",
        "        discounted_prob = 0 # Cannot calculate discounted prob if bigram never occurred\n",
        "\n",
        "    # Calculate the continuation probability P_cont(w3)\n",
        "    # P_cont(w3) is proportional to the number of unique bigrams ending with w3\n",
        "    count_bigrams_ending_with_w3 = len(set((t1, t2) for (t1, t2, t3) in training_trigram_frequencies if t3 == w3))\n",
        "\n",
        "    # Total number of unique bigrams in the training corpus\n",
        "    unique_bigrams_training = set((t1, t2) for (t1, t2, t3) in training_trigram_frequencies)\n",
        "    total_unique_bigrams_training = len(unique_bigrams_training)\n",
        "\n",
        "    if total_unique_bigrams_training > 0:\n",
        "         p_cont_w3 = count_bigrams_ending_with_w3 / total_unique_bigrams_training\n",
        "    else:\n",
        "         p_cont_w3 = 1 / training_vocabulary_size # Fallback if no bigrams exist (unlikely with sufficient data)\n",
        "\n",
        "\n",
        "    # Calculate the interpolation weight (lambda)\n",
        "    if bigram_freq > 0:\n",
        "        lambda_w1_w2 = (discount / bigram_freq) * num_unique_followers\n",
        "    else:\n",
        "        # If bigram_freq is 0, the lambda term is 1, and the probability is solely based on continuation\n",
        "         lambda_w1_w2 = 1\n",
        "\n",
        "    # Combine the two parts using the interpolation weight\n",
        "    smoothed_prob = discounted_prob + lambda_w1_w2 * p_cont_w3\n",
        "\n",
        "    return smoothed_prob if smoothed_prob > 0 else 1e-10 # Return a small positive value for log calculation\n",
        "\n",
        "# 3. Implement a function to calculate perplexity on the test set\n",
        "def calculate_perplexity(test_words, training_trigram_frequencies, training_words):\n",
        "    \"\"\"\n",
        "    Calculates perplexity on a test set using Kneser-Ney smoothed probabilities.\n",
        "\n",
        "    Args:\n",
        "        test_words: List of words in the test corpus.\n",
        "        training_trigram_frequencies: Counter of trigram frequencies from training data.\n",
        "        training_words: List of words in the training corpus.\n",
        "\n",
        "    Returns:\n",
        "        The perplexity score, or None if not enough test trigrams.\n",
        "    \"\"\"\n",
        "    log_prob_sum = 0\n",
        "    n_test_trigrams = 0\n",
        "\n",
        "    # Iterate through trigrams in the test set\n",
        "    for i in range(len(test_words) - 2):\n",
        "        w1, w2, w3 = test_words[i], test_words[i+1], test_words[i+2]\n",
        "\n",
        "        # Calculate smoothed probability P(w3 | w1, w2) using the training model\n",
        "        smoothed_prob = get_kneser_ney_prob(w1, w2, w3, training_trigram_frequencies, training_words)\n",
        "\n",
        "        log_prob_sum += np.log2(smoothed_prob)\n",
        "        n_test_trigrams += 1\n",
        "\n",
        "    if n_test_trigrams > 0:\n",
        "        average_log_prob = log_prob_sum / n_test_trigrams\n",
        "        perplexity = 2 ** (-average_log_prob)\n",
        "        return perplexity\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# 4. Calculate and print the perplexity on the test set.\n",
        "perplexity = calculate_perplexity(test_words, training_trigram_frequencies, training_words)\n",
        "\n",
        "if perplexity is not None:\n",
        "    print(f\"\\nPerplexity on the test set (Kneser-Ney smoothing): {perplexity}\")\n",
        "else:\n",
        "    print(\"\\nNot enough test trigrams to calculate perplexity.\")\n",
        "\n",
        "# 5. Qualitatively assess the model's predictions on a few bigrams from the test set\n",
        "print(\"\\nQualitative assessment of predictions on test set bigrams:\")\n",
        "# Select a few bigrams from the test set to test\n",
        "test_bigrams_for_assessment = []\n",
        "for i in range(len(test_words) - 2):\n",
        "    test_bigrams_for_assessment.append((test_words[i], test_words[i+1], test_words[i+2]))\n",
        "    if len(test_bigrams_for_assessment) >= 10: # Assess the first 10 bigrams in the test set\n",
        "        break\n",
        "\n",
        "# Use the previously defined predict_next_word_kneser_ney function for qualitative assessment\n",
        "# We need to redefine it to use the training data frequencies and words\n",
        "def predict_next_word_kneser_ney_assessment(word1, word2, training_trigram_frequencies, training_words):\n",
        "    \"\"\"\n",
        "    Predicts the next word based on trigram frequencies with Kneser-Ney smoothing,\n",
        "    using the training data.\n",
        "\n",
        "    Args:\n",
        "        word1: The first word of the bigram.\n",
        "        word2: The second word of the bigram.\n",
        "        training_trigram_frequencies: A Counter object containing trigram frequencies from training data.\n",
        "        training_words: The list of words in the training corpus for vocabulary and context counts.\n",
        "\n",
        "    Returns:\n",
        "        The most likely next word based on Kneser-Ney smoothing, or None if no\n",
        "        matching bigrams are found and the vocabulary is empty.\n",
        "    \"\"\"\n",
        "    training_vocabulary = set(training_words)\n",
        "\n",
        "    # Calculate smoothed probabilities for all possible next words\n",
        "    smoothed_probabilities = {}\n",
        "    for w3 in training_vocabulary:\n",
        "        smoothed_probabilities[w3] = get_kneser_ney_prob(word1, word2, w3, training_trigram_frequencies, training_words)\n",
        "\n",
        "    if smoothed_probabilities:\n",
        "        # Find the word with the highest smoothed probability\n",
        "        predicted_word = max(smoothed_probabilities, key=smoothed_probabilities.get)\n",
        "        return predicted_word\n",
        "    else:\n",
        "        return None # Should not happen if vocabulary is not empty\n",
        "\n",
        "\n",
        "for w1, w2, w3_actual in test_bigrams_for_assessment:\n",
        "    predicted_w3 = predict_next_word_kneser_ney_assessment(w1, w2, training_trigram_frequencies, training_words)\n",
        "    print(f\"Bigram: '{w1} {w2}', Actual Next Word: '{w3_actual}', Predicted Next Word: '{predicted_w3}'\")\n",
        "\n",
        "# 6. Discuss the model's performance and identify potential areas for refinement\n",
        "print(\"\\n--- Model Evaluation and Refinement Discussion ---\")\n",
        "print(f\"Perplexity on the test set: {perplexity:.2f}\" if perplexity is not None else \"Perplexity could not be calculated.\")\n",
        "print(\"\\nQualitative Assessment Observations:\")\n",
        "print(\"- Observe how often the predicted word matches the actual next word.\")\n",
        "print(\"- Note if the predicted words seem contextually relevant.\")\n",
        "print(\"- Pay attention to predictions for bigrams that might be rare in the training data but present in the test set.\")\n",
        "\n",
        "print(\"\\nDiscussion and Potential Refinements:\")\n",
        "print(\"1. Perplexity:\")\n",
        "print(f\"   - A perplexity of {perplexity:.2f} (if calculated) indicates how well the model predicts the test data.\" if perplexity is not None else \"   - Perplexity could not be calculated.\")\n",
        "print(\"   - Lower perplexity is better. The value provides a quantitative measure of the model's uncertainty.\")\n",
        "print(\"2. Qualitative Assessment:\")\n",
        "print(\"   - The qualitative assessment provides insights into the types of predictions the model makes.\")\n",
        "print(\"   - Do the predictions make sense in context? Are they generic or specific?\")\n",
        "print(\"3. Limitations and Refinements:\")\n",
        "print(\"   - Corpus Size and Diversity: The current corpus, while more diverse than a single page, is still relatively small.\")\n",
        "print(\"     - Refinement: Train on a much larger and more varied text corpus (e.g., Project Gutenberg texts if installation issues are resolved, or other large datasets).\")\n",
        "print(\"   - Kneser-Ney Smoothing: While better than Add-One, Kneser-Ney has a discount factor that can be tuned.\")\n",
        "print(\"     - Refinement: Implement a method to learn the optimal discount factor from the training data.\")\n",
        "print(\"   - Higher-Order N-grams: Using 4-grams or 5-grams might capture longer-range dependencies but would require significantly more data.\")\n",
        "print(\"     - Refinement: Consider higher-order models if a much larger corpus is available.\")\n",
        "print(\"   - Backoff/Interpolation: Kneser-Ney is a form of interpolation, but more explicit backoff or interpolation with lower-order models (bigrams, unigrams) could be explored.\")\n",
        "print(\"     - Refinement: Formalize the backoff mechanism to ensure robust predictions even for unseen bigrams.\")\n",
        "print(\"   - Handling Out-of-Vocabulary (OOV) Words: Words in the test set not seen in training are currently not handled explicitly.\")\n",
        "print(\"     - Refinement: Implement a strategy for OOV words, such as replacing them with a special <UNK> token during preprocessing or using techniques like character-level models.\")\n",
        "print(\"   - Context Length: Trigrams only consider the previous two words. For complex language, longer context might be beneficial.\")\n",
        "print(\"     - Refinement: Explore neural network-based language models (like LSTMs or Transformers) that can capture longer-range dependencies, although this is a significant shift from n-gram models.\")\n",
        "\n",
        "print(\"\\nOverall, the model's performance is likely limited by the corpus size. Increasing the training data is the most crucial next step. Further refinements like tuning smoothing parameters and exploring more advanced techniques can be pursued after obtaining a larger corpus.\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training set size: 25114 words\n",
            "Test set size: 6279 words\n",
            "Training vocabulary size: 4835\n",
            "\n",
            "Perplexity on the test set (Kneser-Ney smoothing): 12184.195202525192\n",
            "\n",
            "Qualitative assessment of predictions on test set bigrams:\n",
            "Bigram: 'whether the', Actual Next Word: 'past', Predicted Next Word: 'source'\n",
            "Bigram: 'the past', Actual Next Word: 'fully', Predicted Next Word: 'in'\n",
            "Bigram: 'past fully', Actual Next Word: 'determines', Predicted Next Word: 'the'\n",
            "Bigram: 'fully determines', Actual Next Word: 'the', Predicted Next Word: 'the'\n",
            "Bigram: 'determines the', Actual Next Word: 'present', Predicted Next Word: 'the'\n",
            "Bigram: 'the present', Actual Next Word: 'and', Predicted Next Word: 'and'\n",
            "Bigram: 'present and', Actual Next Word: 'what', Predicted Next Word: 'in'\n",
            "Bigram: 'and what', Actual Next Word: 'implications', Predicted Next Word: 'the'\n",
            "Bigram: 'what implications', Actual Next Word: 'this', Predicted Next Word: 'the'\n",
            "Bigram: 'implications this', Actual Next Word: 'would', Predicted Next Word: 'the'\n",
            "\n",
            "--- Model Evaluation and Refinement Discussion ---\n",
            "Perplexity on the test set: 12184.20\n",
            "\n",
            "Qualitative Assessment Observations:\n",
            "- Observe how often the predicted word matches the actual next word.\n",
            "- Note if the predicted words seem contextually relevant.\n",
            "- Pay attention to predictions for bigrams that might be rare in the training data but present in the test set.\n",
            "\n",
            "Discussion and Potential Refinements:\n",
            "1. Perplexity:\n",
            "   - A perplexity of 12184.20 (if calculated) indicates how well the model predicts the test data.\n",
            "   - Lower perplexity is better. The value provides a quantitative measure of the model's uncertainty.\n",
            "2. Qualitative Assessment:\n",
            "   - The qualitative assessment provides insights into the types of predictions the model makes.\n",
            "   - Do the predictions make sense in context? Are they generic or specific?\n",
            "3. Limitations and Refinements:\n",
            "   - Corpus Size and Diversity: The current corpus, while more diverse than a single page, is still relatively small.\n",
            "     - Refinement: Train on a much larger and more varied text corpus (e.g., Project Gutenberg texts if installation issues are resolved, or other large datasets).\n",
            "   - Kneser-Ney Smoothing: While better than Add-One, Kneser-Ney has a discount factor that can be tuned.\n",
            "     - Refinement: Implement a method to learn the optimal discount factor from the training data.\n",
            "   - Higher-Order N-grams: Using 4-grams or 5-grams might capture longer-range dependencies but would require significantly more data.\n",
            "     - Refinement: Consider higher-order models if a much larger corpus is available.\n",
            "   - Backoff/Interpolation: Kneser-Ney is a form of interpolation, but more explicit backoff or interpolation with lower-order models (bigrams, unigrams) could be explored.\n",
            "     - Refinement: Formalize the backoff mechanism to ensure robust predictions even for unseen bigrams.\n",
            "   - Handling Out-of-Vocabulary (OOV) Words: Words in the test set not seen in training are currently not handled explicitly.\n",
            "     - Refinement: Implement a strategy for OOV words, such as replacing them with a special <UNK> token during preprocessing or using techniques like character-level models.\n",
            "   - Context Length: Trigrams only consider the previous two words. For complex language, longer context might be beneficial.\n",
            "     - Refinement: Explore neural network-based language models (like LSTMs or Transformers) that can capture longer-range dependencies, although this is a significant shift from n-gram models.\n",
            "\n",
            "Overall, the model's performance is likely limited by the corpus size. Increasing the training data is the most crucial next step. Further refinements like tuning smoothing parameters and exploring more advanced techniques can be pursued after obtaining a larger corpus.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "pip install gutenberg"
      ],
      "metadata": {
        "id": "tAIcXYFWj11l"
      }
    }
  ]
}